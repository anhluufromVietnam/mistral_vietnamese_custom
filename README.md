# mistral_vietnamese_custom with llama_cpp GPU Benchmark & Chat Interface

This project provides a complete setup for benchmarking, plotting, and chatting with `llama_cpp` using GPU acceleration (CUDA-only, no cuDNN required). It includes:

- `benchmark.py`: Benchmark llama models with different `ctx`, `gpu_layers`, and `batch` settings.
- `plotmat.py`: Visualize the benchmark results.
- `AI.py`: Interact with the model through a terminal chat interface.
- `testgpu.py`: Verify GPU status and VRAM usage.

---

## ğŸš€ Installation

### âœ… Requirements

- Windows 10/11 with an **NVIDIA GPU**
- Python 3.9+
- CUDA Toolkit 12.x (cuDNN **not required**)
- Visual Studio C++ Build Tools

### ğŸ”§ Setup Instructions

1. **Install Python**  
   Download from: https://www.python.org/downloads/

2. **Install Visual Studio Build Tools**  
   - Get it here: https://visualstudio.microsoft.com/visual-cpp-build-tools/
   - Select: **Desktop Development with C++**

3. **Install CUDA Toolkit**  
   Download from: https://developer.nvidia.com/cuda-downloads

4. **Ensure Latest NVIDIA GPU Drivers**  
   Update from: https://www.nvidia.com/Download/index.aspx

5. **Clone this repository**  
   ```bash
   git clone [https://github.com/anhluufromVietnam/mistral_vietnamese_custom.git]
   cd mistral_vietnamese_custom
Install dependencies

# Llama Benchmark GPU

## Installation

1. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

2. Install `llama_cpp` (CUDA-enabled):
   If you have the `.whl` file (`llama_cpp_python-0.3.4-cp39-cp39-win_amd64.whl`), install it directly:
   ```bash
   pip install llama_cpp_python-0.3.4-cp39-cp39-win_amd64.whl
   ```

## ğŸ“ Scripts & Usage

### ğŸ§ª `benchmark.py`
Run benchmarks for multiple configuration combinations.
```bash
python benchmark.py
```
This will:
- Run inference with various context sizes (`ctx`), GPU layers (`gpu_layers`), and batch sizes.
- Save results to `benchmark_output.csv`.
- Automatically release GPU memory between runs.

### ğŸ“ˆ `plotmat.py`
Visualize benchmark results as graphs.
```bash
python plotmat.py
```
Youâ€™ll get:
- One subplot per context size.
- Lines showing how response time varies by batch and GPU layers.
- Colorful, connected lines for easy comparison.

### ğŸ’¬ `AI.py`
Simple chat interface with the model.
```bash
python AI.py
```
You can enter your questions and get AI responses powered by the local `llama_cpp` backend.

### ğŸ§  `testgpu.py`
Check whether your GPU is active and which processes are using VRAM.
```bash
python testgpu.py
```

## ğŸ“Š Benchmark Results Summary
Based on the `benchmark_output.csv` data, here are average response times:
### ğŸ§  Benchmark Results Summary for 16-bit floating point precision

| Context Size | GPU Layers | Batch Size  | Avg Response Time (sec) |
|--------------|------------|-------------|--------------------------|
| 2048         | 20         | 256â€“1024    | ~23.1                    |
| 2048         | 24         | 256â€“1024    | ~15.5                    |
| 2048         | 28         | 256â€“1024    | ~12.2                    |
| 2048         | 32         | 256â€“1024    | ~15.2                    |
| 4096â€“16384   | 28         | 512â€“1024    | ~12.2                    |
| 32768        | >28        | Any         | âŒ Failed (VRAM exceeded) |
### ğŸ§  Benchmark Results Summary for 4-bit Quantization
| Context Size | GPU Layers | Batch Size  | Avg Response Time (sec) |
|--------------|------------|-------------|--------------------------|
| 2048         | 20         | 256â€“1024    | ~7.3                     |
| 2048         | 24         | 256â€“1024    | ~5.7                     |
| 2048         | 28         | 256â€“1024    | ~3.6                     |
| 2048         | 32         | 256â€“1024    | ~2.0                     |
| 4096         | 28         | 512â€“1024    | ~3.6                     |
| 8192         | 32         | 256â€“1024    | ~2.0                     |
| 16384        | 32         | 256â€“1024    | ~2.0                     |
| 32768        | 32         | 256â€“1024    | ~2.0                     |

## âœ… Best Performance
### for 16-bit floating point precision
- **GPU Layers**: 28
- **Batch**: 1024
- **Ctx**: 2048 to 16384
- Delivers ~12.2 sec response time on average.

## âš ï¸ Limitations
- At `ctx=32768`, models with 28+ GPU layers fail due to memory exhaustion (~75MB allocation failure).
- Increase in GPU layers beyond 28 does not always yield better performance and may reduce VRAM headroom.

## âœ… Best Performance
### for 4-bit quantization (q4_1)
GPU Layers: 32
Batch: 256 to 1024
Ctx: 2048 to 32768
Delivers ~2.0 sec response time on average.

## âš ï¸ Limitations
At ctx=32768, models with more than 28 GPU layers fail due to memory exhaustion.
Response times may vary with lower GPU layers, indicating potential trade-offs in performance.

### Response Time Summary Table

```markdown
| Context Size | GPU Layers | Batch Size | Avg Response Time (sec) |
|--------------|------------|------------|--------------------------|
| 16384        | 32         | 512        | 2.09                     |
| 16384        | 32         | 1024       | 2.02                     |
| 16384        | 36         | 512        | 1.67                     |
| 16384        | 36         | 1024       | 1.66                     |
| 16384        | 40         | 512        | 1.66                     |
| 16384        | 40         | 1024       | 1.66                     |
| 16384        | 44         | 512        | 1.66                     |
| 16384        | 44         | 1024       | 1.67                     |
| 16384        | 48         | 512        | 1.67                     |
| 16384        | 48         | 1024       | 1.67                     |
| 32768        | 32         | 512        | 2.05                     |
| 32768        | 32         | 1024       | 2.05                     |
| 32768        | 36         | 512        | 1.69                     |
| 32768        | 36         | 1024       | 1.69                     |
| 32768        | 40         | 512        | 1.69                     |
| 32768        | 40         | 1024       | 1.70                     |
| 32768        | 44         | 512        | 1.69                     |
| 32768        | 44         | 1024       | 1.69                     |
| 32768        | 48         | 512        | 1.70                     |
| 32768        | 48         | 1024       | 1.69                     |
```

### Conclusion

## âœ… Optimal Performance
- **GPU Layers**: 44 and 48
- **Batch**: 1024
- **Ctx**: 16384 to 32768
- Delivers response times around **1.66 to 1.67 seconds**.

## ğŸ“Š Key Insights
- **Impact of Batch Size**: Increasing batch size to 1024 improves response times, particularly at higher GPU layers.
- **Scalability**: As context size increases, response times stabilize, indicating efficient scalability for larger inputs.
- **Layer Efficiency**: Performance gains plateau beyond 36 GPU layers, suggesting diminishing returns with higher layers.

In summary, using 44 or 48 GPU layers with a batch size of 1024 provides the best performance balance for larger context sizes.

Here's a GitHub-style **README markdown** with a compact summary table of your benchmark results using **Mistral 7B (4-bit quantized, 1 GPU)**. The table format follows your example, and includes key groupings based on context size, GPU layers, and batch size.

---
Here is your full **GitHub README-style** content including the conclusion and benchmark table, formatted for clean copy-paste into a `README.md` file:

---

# ğŸ§  Vistral 7B (4Q\_1) Inference Benchmark

This document presents benchmark results evaluating **response time** across different context sizes, batch sizes, and GPU layer offloading levels using **Vistral 7B quantized to 4-bit (4Q\_1)**.

---

## ğŸ“Š Conclusion from the Graph

### Response Time vs Context Size:

* Response time is relatively **stable** across context sizes (16,384 vs 32,768).
* Slightly **higher response time** occurs at lower context sizes for **smaller batch sizes** and **fewer GPU layers**.

### Effect of Batch Size:

* **Batch 2048** generally results in **slightly lower response times** than batch 1024.

### Effect of GPU Layers:

* Increasing GPU layers (e.g., 128 â†’ 1024) **slightly reduces response time**, especially for **larger batches**.

---

## ğŸ“‹ Benchmark Summary Table

| Context Size | GPU Layers | Batch Size | Avg Response Time (sec)  |
| ------------ | ---------- | ---------- | ------------------------ |
| 16384        | 64         | 1024â€“2048  | \~1.71                   |
| 16384        | 128        | 1024â€“2048  | \~1.67                   |
| 16384        | 256        | 1024â€“2048  | \~1.67                   |
| 16384        | 1024       | 2048       | \~1.66                   |
| 32768        | 256        | 1024â€“2048  | \~1.69                   |
| 32768        | 1024       | 1024â€“2048  | \~1.70                   |
| 32768        | >1024      | Any        | âŒ Failed (VRAM exceeded) |

---


## ğŸ“Œ File Structure
```bash
llama-benchmark-gpu/
â”œâ”€â”€ benchmark.py                      # Run benchmark tests
â”œâ”€â”€ plotmat.py                        # Plot results from benchmark_output.csv
â”œâ”€â”€ AI.py                             # Interactive chat with LLM
â”œâ”€â”€ testgpu.py                        # GPU usage check
â”œâ”€â”€ benchmark_output.csv
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ ggml-vistral-7B-chat-f16.gguf     # Artifacts model file
â””â”€â”€ README.md
```

## ğŸ§  Model Info & Credits

We use the **[Vistral-7B-Chat](https://huggingface.co/Vistral-7B-Chat-GGUF)** model, a cutting-edge Vietnamese large language model based on the **Mistral architecture**.

### ğŸ“Œ Model Overview:
- **Architecture**: Based on [Mistral-7B](https://arxiv.org/abs/2310.06825), adapted for Vietnamese.
- **Training**: Fine-tuned with high-quality Vietnamese instructions and conversation data.
- **Use Case**: Chatbot applications, question answering, summarization, and general Vietnamese NLP tasks.
- **Format**: GGUF format for efficient inference with `llama.cpp` and `llama-cpp-python`.

### ğŸ§¾ Citation

If you use this model in your work, please cite:

```bibtex
@article{chien2023vistral,
  author = {Chien Van Nguyen, Thuat Nguyen, Quan Nguyen, Huy Huu Nguyen, BjÃ¶rn PlÃ¼ster, Nam Pham, Huu Nguyen, Patrick Schramowski, Thien Huu Nguyen},
  title = {Vistral-7B-Chat - Towards a State-of-the-Art Large Language Model for Vietnamese},
  year = 2023,
}


ğŸ“ Contact
For questions or contributions, please open an issue or PR.
