# mistral_vietnamese_custom with llama_cpp GPU Benchmark & Chat Interface

This project provides a complete setup for benchmarking, plotting, and chatting with `llama_cpp` using GPU acceleration (CUDA-only, no cuDNN required). It includes:

- `benchmark.py`: Benchmark llama models with different `ctx`, `gpu_layers`, and `batch` settings.
- `plotmat.py`: Visualize the benchmark results.
- `AI.py`: Interact with the model through a terminal chat interface.
- `testgpu.py`: Verify GPU status and VRAM usage.

---

## ğŸš€ Installation

### âœ… Requirements

- Windows 10/11 with an **NVIDIA GPU**
- Python 3.9+
- CUDA Toolkit 12.x (cuDNN **not required**)
- Visual Studio C++ Build Tools

### ğŸ”§ Setup Instructions

1. **Install Python**  
   Download from: https://www.python.org/downloads/

2. **Install Visual Studio Build Tools**  
   - Get it here: https://visualstudio.microsoft.com/visual-cpp-build-tools/
   - Select: **Desktop Development with C++**

3. **Install CUDA Toolkit**  
   Download from: https://developer.nvidia.com/cuda-downloads

4. **Ensure Latest NVIDIA GPU Drivers**  
   Update from: https://www.nvidia.com/Download/index.aspx

5. **Clone this repository**  
   ```bash
   git clone [https://github.com/anhluufromVietnam/mistral_vietnamese_custom.git]
   cd mistral_vietnamese_custom
Install dependencies

bash
Copy
Edit
pip install -r requirements.txt
Install llama_cpp (CUDA-enabled)
If you have the .whl file (llama_cpp_python-0.3.4-cp39-cp39-win_amd64.whl), install it directly:

bash
Copy
Edit
pip install llama_cpp_python-0.3.4-cp39-cp39-win_amd64.whl
ğŸ“ Scripts & Usage
ğŸ§ª benchmark.py
Run benchmarks for multiple configuration combinations.

bash
Copy
Edit
python benchmark.py
This will:

Run inference with various context sizes (ctx), GPU layers (gpu_layers), and batch sizes

Save results to benchmark_output.csv

Automatically release GPU memory between runs

ğŸ“ˆ plotmat.py
Visualize benchmark results as graphs.

bash
Copy
Edit
python plotmat.py
Youâ€™ll get:

One subplot per context size

Lines showing how response time varies by batch and GPU layers

Colorful, connected lines for easy comparison

ğŸ’¬ AI.py
Simple chat interface with the model.

bash
Copy
Edit
python AI.py
You can enter your questions and get AI responses powered by the local llama_cpp backend.

ğŸ§  testgpu.py
Check whether your GPU is active and which processes are using VRAM.

bash
Copy
Edit
python testgpu.py
ğŸ“Š Benchmark Results Summary
Based on the benchmark_output.csv data, here are average response times:

Context Size	GPU Layers	Batch Size	Avg Response Time (sec)
2048	20	256â€“1024	~23.1
2048	24	256â€“1024	~15.5
2048	28	256â€“1024	~12.2
2048	32	256â€“1024	~15.2
4096â€“16384	28	512â€“1024	~12.2
32768	>28	Any	âŒ Failed (VRAM exceeded)

âœ… Best Performance
GPU Layers = 28

Batch = 1024

Ctx = 2048 to 16384

Delivers ~12.2 sec response time on average.

âš ï¸ Limitations
At ctx=32768, models with 28+ GPU layers fail due to memory exhaustion (~75MB allocation failure).

Increase in GPU layers beyond 28 does not always yield better performance and may reduce VRAM headroom.

ğŸ“Œ File Structure
bash
Copy
Edit
llama-benchmark-gpu/
â”œâ”€â”€ benchmark.py        # Run benchmark tests
â”œâ”€â”€ plotmat.py          # Plot results from benchmark_output.csv
â”œâ”€â”€ AI.py               # Interactive chat with LLM
â”œâ”€â”€ testgpu.py          # GPU usage check
â”œâ”€â”€ benchmark_output.csv
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ ggml-vistral-7B-chat-f16.gguf # artifacts model file
â””â”€â”€ README.md

## ğŸ§  Model Info & Credits

We use the **[Vistral-7B-Chat](https://huggingface.co/Vistral-7B-Chat-GGUF)** model, a cutting-edge Vietnamese large language model based on the **Mistral architecture**.

### ğŸ“Œ Model Overview:
- **Architecture**: Based on [Mistral-7B](https://arxiv.org/abs/2310.06825), adapted for Vietnamese.
- **Training**: Fine-tuned with high-quality Vietnamese instructions and conversation data.
- **Use Case**: Chatbot applications, question answering, summarization, and general Vietnamese NLP tasks.
- **Format**: GGUF format for efficient inference with `llama.cpp` and `llama-cpp-python`.

### ğŸ§¾ Citation

If you use this model in your work, please cite:

```bibtex
@article{chien2023vistral,
  author = {Chien Van Nguyen, Thuat Nguyen, Quan Nguyen, Huy Huu Nguyen, BjÃ¶rn PlÃ¼ster, Nam Pham, Huu Nguyen, Patrick Schramowski, Thien Huu Nguyen},
  title = {Vistral-7B-Chat - Towards a State-of-the-Art Large Language Model for Vietnamese},
  year = 2023,
}


ğŸ“ Contact
For questions or contributions, please open an issue or PR.
